{
  "title": "goosego.log",
  "link": "https://velog.io/",
  "items": [
    {
      "title": " TIL: LangGraph 기초",
      "link": "https://velog.io/@goosego/TIL-LangGraph-%EA%B8%B0%EC%B4%88",
      "pubDate": "Thu, 11 Sep 2025 08:32:02 GMT",
      "contentSnippet": "2025-09-11 1. LangChain vs LangGraph 비교 구분 LangChain LangGraph 주요 목적 외부 도구와 빠른 통합, 간단한 체인 구조로 앱 개발, 프로토타이핑에 적합 복잡한 워크플로우, 조건부 로직, 다단계 의사결정 프로세스 구현에 적합 구조 체인/에이전트 기반 구조, 선형적 처리 그래프 기반 구조, 노드와 엣지로 유연한 설계 가능 상태 관리 암시적·자동화된 상태 관리, 단순 개발 명시적·세밀한 상태 관리, 각 단계 상태 추적 가능 유연성 미리 정의된 컴포넌트 중심, 빠른 구현 가능 그래프 구조 자유 설계, 노드 단위 세부 제어 가능 학습 곡선 직관적 API, 예제가 많아 러닝 커브 완만 그래프 이론·상태 관리 필요, 러닝 커브 가파름 용도 간단한 LLM 앱, RAG, 빠른 프로토타이핑 복잡한 AI 시스템, 다중 에이전트, 고급 프로젝트 2. Agent 개념 Agent 정의: LLM이 단순 답변을 넘어서 스스로 판단 → 도구 실행 → 결과 활용하는 구조 핵심 요소 4가지 Goal(목표): 사용자의 요청 Policy(정책): 어떤 행동을 취할지 결정 Tools(도구): 계산기, 검색기, DB, 코드 실행 등 Memory(메모리): 대화 기록, 실행 로그 Agent Loop 기본 흐름 질문 → 생각 → 도구 실행 → 관찰 → 반복 → 최종 답변 LangGraph와의 연결 Agent 루프 = 보이지 않는 흐름 LangGraph = 이를 그래프로 시각화/제어하는 프레임워크 Node = 생각 / 실행 / 판단 Edge = 단계 이동 규칙 State = 현재 맥락 Checkpoint = 중단·재개 지원 3. LangGraph의 필요성과 탄생 배경 단순 LLM 답변에는 환각(Hallucination) 이 포함될 수 있음 RAG 답변이 문서에 없을 때 사전 지식으로 부정확하게 답변하는 문제가 발생 해결: 검색에서 원하는 내용이 없으면 “없음”으로 처리 부족한 정보를 추가 검색하여 보강 즉, LangGraph = 복잡한 RAG/에이전트 플로우를 그래프로 설계하고 관리하는 기반 도구 4. 오늘 느낀 점 LangChain은 빠른 시작, LangGraph는 복잡한 구조 설계에 적합 Agent를 단순 개념이 아닌 루프와 그래프로 이해해야 활용도가 커짐"
    },
    {
      "title": "TIL (2025-09-09) — 평가지표 정리",
      "link": "https://velog.io/@goosego/TIL-2025-09-09-%ED%8F%89%EA%B0%80%EC%A7%80%ED%91%9C-%EC%A0%95%EB%A6%AC",
      "pubDate": "Tue, 09 Sep 2025 10:26:24 GMT",
      "contentSnippet": "주제: LLM 생성 품질 지표 + RAG 전용 지표(RAGAS) 1) 큰 그림(개요) 평가(Evaluation) = 모델이 만든 출력의 품질을 수치로 보는 과정 생성 품질 지표(요약/번역/문장 유사): ROUGE, BLEU, METEOR, 임베딩 기반(=SemScore) RAG 전용 지표: context_precision, context_recall, faithfulness, answer_relevancy 2) 공통 개념 먼저 정확도(accuracy) 같은 분류 지표만으로는 생성형 작업의 품질을 설명하기 어렵다. 생성형 평가는 보통 참조 텍스트(reference) 와 생성 텍스트(candidate) 를 비교한다. 용어 정리 n-그램: 연속된 n개의 토큰 토크나이저: 문장을 토큰으로 자르는 함수 (한국어는 Kiwi 등 권장) 코사인 유사도: 두 벡터 a, b의 각도를 기반으로 한 유사도 수식(ASCII): cos_sim = (a·b) / (||a|| · ||b||) 3) 생성 품질 지표 3-1) ROUGE (요약/문서 커버리지) 참조와 생성 사이의 n-그램 중첩과 LCS(최장 공통 부분 수열) 기반 ROUGE-1/2: 단어 겹침 정도, ROUGE-L: 순서까지 반영 해석 가이드 ROUGE-L ≈ 0.5 → 핵심어 절반 이상 커버 ROUGE ≥ 0.7 → 커버리지 양호 주의: 의미는 다르지만 단어만 겹쳐도 점수가 높을 수 있음 예시 코드 from rouge_score import rouge_scorer scorer = rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL']) ref = \"안녕하세요. 반갑습니다. 제 이름은 최영화입니다.\" hyp = \"안녕하세요 반갑습니다 제 이름은 최영화 입니다\" print(scorer.score(ref, hyp)) 3-2) BLEU (기계번역 전통 지표) n-그램 정밀도(precision) + 길이 패널티 표현이 조금만 달라져도 점수가 급격히 하락 → 스무딩 필수 예시 코드 from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction ref = [\"안녕하세요\",\"반갑습니다\",\"제\",\"이름은\",\"최영화입니다\",\".\"] hyp = [\"안녕하세요\",\"반갑습니다\",\"제\",\"이름은\",\"최영화입니다\",\"!\"] score = sentence_bleu([ref], hyp, smoothing_function=SmoothingFunction().method3) print(score) 3-3) METEOR (동의어·형태소 반영) BLEU 보완 지표 → 동의어, 어간, 형태 변화 고려 인간 평가와 상관도가 높음 단점: WordNet 같은 언어 자원 필요, 한국어에서는 제한적 예시 코드 from nltk.translate.meteor_score import meteor_score meteor_score([\"run quickly\"], \"running fast\") 3-4) SemScore (임베딩 기반) 문장 임베딩 후 코사인 유사도로 의미 유사도 직접 측정 표현이 달라도 의미가 같으면 높은 점수 임베딩 모델 품질에 의존 예시 코드 from sentence_transformers import SentenceTransformer, util model = SentenceTransformer(\"all-mpnet-base-v2\") a = model.encode(\"안녕하세요 반갑습니다\", convert_to_tensor=True) b = model.encode(\"만나서 반갑습니다 안녕하세요\", convert_to_tensor=True) print(util.pytorch_cos_sim(a,b).item()) 4) RAG 전용 지표 (RAGAS) RAG 파이프라인은 검색 품질과 생성 품질을 분리해 평가해야 한다. context_precision: 가져온 문맥 중 실제 관련 있는 비율 context_recall: 필요한 문맥을 빠짐없이 가져왔는가 faithfulness: 답변이 근거 문맥에 충실했는가 (환각 방지) answer_relevancy: 답변이 질문에 얼마나 충실한가 데이터셋 포맷 question, reference, answer, retrieved_contexts 4개 컬럼 필요 예시 코드 from datasets import Dataset from ragas import evaluate from ragas.metrics import faithfulness, answer_relevancy, context_recall, context_precision data = { \"question\": [\"Q1\",\"Q2\"], \"reference\": [\"gold1\",\"gold2\"], \"answer\": [\"ans1\",\"ans2\"], \"retrieved_contexts\": [[\"ctx1\",\"ctx2\"],[\"ctx3\"]] } df = Dataset.from_dict(data) print(evaluate(df, metrics=[context_precision, context_recall, faithfulness, answer_relevancy]).to_pandas()) 해석 기준(경험치) precision ≥ 0.6 → 잡음 과도하지 않음 recall ≥ 0.6 → 필요한 근거 상당수 검색됨 faithfulness ≥ 0.7 → 근거 기반 생성 양호 relevancy ≥ 0.7 → 질문 충실도 양호 5) 언제 무엇을 쓸까? 요약 커버리지 → ROUGE-L 번역 유사도 → BLEU (+스무딩) 의미 동등성 → SemScore, METEOR RAG 전체 점검 → RAGAS 4종 (검색 vs 생성 분리)"
    },
    {
      "title": "TIL: Local LLM 연동",
      "link": "https://velog.io/@goosego/TIL-Local-LLM-%EC%97%B0%EB%8F%99",
      "pubDate": "Fri, 05 Sep 2025 08:17:17 GMT",
      "contentSnippet": "2025-09-05 Local LLM 활용 Local LLM: 로컬 컴퓨터에 대형 언어 모델을 설치하고 실행하는 방식 장점 비용 절감 (클라우드 API 호출비 없음) 보안 강화 (데이터가 외부 서버로 나가지 않음) 단점 성능이 부족할 수 있음 (특히 GPU 필요) Ollama 로컬 환경에서 LLM을 실행할 수 있도록 설계된 오픈소스 툴 다양한 모델 지원 Ollama LMStudio Llamma 등 설치 및 실행 https://ollama.com/download 에서 다운로드 설치 후 챗 화면 자동 실행 원하는 모델을 선택하고 메시지를 전송하면 모델 다운로드 시작 다운로드 완료 후 해당 모델을 이용해 챗봇 동작 가능 일부 모델은 Turbo 모드 제공 → 빠른 응답 가능 Local 모델 + Gemma3 + LangChain 연동 로컬 모델을 Gemma3 기반 LangChain과 연결해 활용 가능 체인 안에 로컬 LLM을 포함시켜 워크플로우 구성 간단한 실습 PromptTemplate 사용해 체인 연결 구조: prompt → llm → StrOutputParser() Message System 활용 역할(Role)과 출력값 형태를 지정 가능 메모리 기능 부여 대화 맥락을 유지하는 챗봇 구현 오늘의 정리 Local LLM은 비용·보안 측면에서 강점이 있지만 GPU 리소스가 필요 Ollama 같은 툴을 활용하면 다양한 모델을 손쉽게 실행 가능 LangChain과 연동해 로컬 모델을 체인 기반 챗봇에 활용할 수 있음"
    },
    {
      "title": "TIL: RAG 실습 (이미지 검색) - 2025.09.03",
      "link": "https://velog.io/@goosego/TIL-RAG-%EC%8B%A4%EC%8A%B5-%EC%9D%B4%EB%AF%B8%EC%A7%80-%EA%B2%80%EC%83%89-2025.09.03",
      "pubDate": "Wed, 03 Sep 2025 08:37:14 GMT",
      "contentSnippet": "RAG 기술을 활용하여 이미지 정보추출 챗봇 만들기 멀티모달 모델 사용 (이미지, 텍스트) 프롬프트 입력으로 텍스트뿐 아니라 이미지, 오디오 등 다양한 데이터를 사용할 수 있는 모델 gpt-4o, gpt-4-turbo → 이미지 인식(Vision) 기능 추가 사용 방법 OpenAI SDK 활용 (공식) LangChain + teddynote MultiModal 활용 teddynote의 MultiModal() 함수 내부적으로 설계되어 있어서 함수 형태로 가져다 쓰면 편리 장점: 간편하게 활용 가능 단점: 커스터마이징이 어려움 OpenAI SDK 활용한 멀티모달 실습 막대 그래프, 원 그래프를 이미지로 넣어 분석 결과 출력 도로 위의 교통 표지판 이미지를 분석해 텍스트 추출 오늘의 정리 RAG(검색 기반 생성)과 멀티모달 모델을 결합하면 텍스트 + 이미지 입력이 가능한 챗봇을 구현할 수 있음 LangChain과 SDK 두 가지 방식으로 접근 가능 간단한 사용은 teddynote MultiModal 함수가 편리하지만, 세밀한 제어는 OpenAI SDK가 더 유리함"
    },
    {
      "title": "TIL (Today I Learned) - 2025-09-01",
      "link": "https://velog.io/@goosego/TIL-Today-I-Learned-2025-09-01",
      "pubDate": "Tue, 02 Sep 2025 07:27:24 GMT",
      "contentSnippet": "1. LangChain 기반 ChatBot 구현 학습 목표 간단한 챗봇을 구성하고 메모리(memory) 기능을 통해 대화 맥락을 유지하는 방법을 학습 주요 개념 ConversationChain LLM에 메모리를 붙여 대화 맥락을 유지하는 기능 LangChain 0.2.7 이후 deprecated → 향후 1.0에서 제거 예정 Memory 종류 Memory Type 설명 ConversationBufferMemory 모든 대화를 그대로 저장 ConversationBufferWindowMemory 최근 k개의 대화만 저장 ConversationTokenBufferMemory 토큰 길이를 기준으로 유지 ConversationSummaryMemory 대화를 요약해서 저장 ConversationSummaryBufferMemory 요약 + 최근 대화 일부 유지 VectorStoreRetrieverMemory 대화를 벡터 DB에 저장 후 검색 RunnableWithMessageHistory 최신 LangChain에서 권장되는 방식 세션 단위 관리 (session_id) 과거 메시지 불러오기 → 답변 생성 → 히스토리에 기록 대화 맥락을 효율적으로 관리 2. RAG (Retrieval-Augmented Generation) - PDF 문서 검색 실습 개념 RAG = Retrieval + Augmented + Generation 외부 문서를 검색해 활용하고, 그 근거를 바탕으로 답변 생성 구조 Retrieval (검색): 질문과 관련된 데이터 검색 Augmented (증강): 검색된 데이터 임베딩 → 벡터 DB 저장 Generation (생성): 검색 결과를 기반으로 답변 생성 장점 풍부하고 구체적인 답변 최신 정보 반영 환각(Hallucination) 방지 실습 프로세스 사전 준비: PDF 로드 → 텍스트 분할(Chunking) → 임베딩 → 벡터 DB 저장 실행: 검색기 설정 → 프롬프트 구성 → LLM 연결 3. RAG (Retrieval-Augmented Generation) - 웹 페이지 검색 실습 목표 PDF 대신 웹 페이지(네이버 뉴스 등) 를 대상으로 RAG 기반 QA 챗봇 구현 주요 라이브러리 라이브러리/모듈 기능 bs4 (BeautifulSoup) 웹 페이지 HTML 파싱 후 텍스트 추출 WebBaseLoader 웹 텍스트를 LangChain 문서 객체로 변환 RecursiveCharacterTextSplitter 문서 → 문단 → 문장 단위로 분할 OpenAIEmbeddings 텍스트를 벡터로 변환 FAISS 벡터 저장 및 유사도 검색 PromptTemplate 입력값을 프롬프트로 변환 RunnablePassthrough 입력값을 그대로 다음 단계로 전달 StrOutputParser LLM 출력 후처리 ChatOpenAI OpenAI GPT 모델 연결 프로세스 웹 페이지 크롤링 → 텍스트 추출 문서를 Chunk 단위로 분리 임베딩 변환 후 FAISS 벡터 DB 저장 검색기(retriever) 구성 프롬프트 템플릿 정의 → LLM 연결 질문 입력 → 검색 결과 반영 → 최종 응답 생성 특징 PDF 실습과 구조는 동일하지만 데이터 소스가 웹 페이지 동적으로 바뀌는 뉴스 기사 같은 최신 데이터를 활용 가능 4. 오늘의 배움 요약 LangChain 메모리 관리: 대화 맥락 유지 가능 (버퍼/요약/벡터 기반 등 다양한 방식 존재) RAG (PDF 실습): 문서 기반 질의응답 챗봇 구현 가능 RAG (웹 실습): 웹 페이지 크롤링 후, 최신 데이터를 반영한 QA 챗봇 구현 가능 결론: 대화 맥락 유지 + 외부 데이터 검색을 결합하면, 훨씬 실용적이고 똑똑한 챗봇을 만들 수 있음"
    },
    {
      "title": "TIL (Today I Learned)",
      "link": "https://velog.io/@goosego/TIL-Today-I-Learned",
      "pubDate": "Mon, 01 Sep 2025 07:01:47 GMT",
      "contentSnippet": "2025-09-01 1. LangChain 기반 ChatBot 구현 학습 목표 간단한 챗봇을 구성하고 메모리(memory) 기능을 통해 대화 맥락을 유지하는 방법을 학습 주요 개념 ConversationChain LLM에 메모리를 붙여 대화 맥락을 유지하는 기능 하지만 LangChain 0.2.7 버전부터 deprecated 되었으며, 향후 1.0 버전에서 제거 예정 Memory 종류 Memory Type 설명 ConversationBufferMemory 모든 대화 내용을 그대로 저장 ConversationBufferWindowMemory 최근 k개의 대화만 저장 ConversationTokenBufferMemory 토큰 길이를 기준으로 대화 유지 ConversationSummaryMemory 대화 내용을 요약해서 저장 ConversationSummaryBufferMemory 요약 + 최근 대화 일부 유지 VectorStoreRetrieverMemory 대화를 벡터 DB에 저장하고, 질문 시 관련 내용 검색 RunnableWithMessageHistory 최신 LangChain에서 권장되는 대화 기록 관리 방식 세션 기반 관리: 사용자별 session_id를 기준으로 메시지를 구분 Load Messages: 과거 대화 불러오기 Save Response: 생성된 답변을 기록해 이후 대화 맥락으로 활용 Outputs: 모델의 최종 응답 정리 단순히 모델만 호출하는 것보다 대화 맥락 관리 전략을 세우는 것이 중요 버퍼 저장, 요약 저장, 벡터 검색 등 다양한 방식으로 효율과 비용을 조절 가능 앞으로는 RunnableWithMessageHistory 방식 사용이 권장됨 2. RAG (Retrieval-Augmented Generation) 실습 개념 RAG = Retrieval + Augmented + Generation LLM이 학습 데이터 외부의 실제 문서를 검색하고 활용하여 답변을 생성하는 구조 기본 구조 Retrieval (검색 단계) 질문과 관련된 외부 데이터를 검색 Augmented (증강 단계) 데이터를 임베딩 후 벡터 DB에 저장 검색 효율을 높이기 위한 전처리 과정 Generation (생성 단계) 검색된 정보를 LLM과 함께 활용해 답변 생성 장점 풍부한 정보 제공: 단순 모델 응답보다 더 구체적임 실시간 정보 반영: 최신 데이터를 반영 가능 환각 방지: 근거 문서 기반 응답으로 신뢰도 상승 실습 프로세스 사전 준비 단계 PDF 파일 로드 (Load Data) 텍스트 분할 (Text Split → Chunking) 임베딩 변환 및 벡터 DB 저장 실행 단계 검색기 설정 (Retriever) 프롬프트 구성 LLM과 연결해 최종 질의응답 실행 배운 점 단순 LLM보다, RAG 구조를 붙여야 실제 활용성 높은 챗봇을 만들 수 있음 특히 특정 문서 기반 Q&A 챗봇을 제작하는 데 효과적임 3. 오늘의 배움 요약 LangChain의 메모리 관리 기능을 통해 챗봇이 대화 맥락을 유지할 수 있음을 학습 RAG 구조를 통해 외부 문서 기반 검색 + 생성형 응답이 가능함을 확인 두 가지를 결합하면, 맥락을 이해하면서 외부 지식까지 활용하는 고도화된 챗봇을 만들 수 있음"
    },
    {
      "title": "Today I Learn",
      "link": "https://velog.io/@goosego/Today-I-Learn",
      "pubDate": "Fri, 29 Aug 2025 10:27:35 GMT",
      "contentSnippet": "25.08.29 LangChain 다양한 활용법 제로샷 프롬프팅 (Zero-Shot Prompting) Zero-Shot: AI 모델이 사전 지식 없이 새로운 답변을 출력하는 방식 Zero-Shot-Prompting: 사전 예시 없이, 단순 요청에 모델이 답변을 생성하는 방식 특징 모델은 오로지 프롬프트 템플릿에 포함된 정보만을 기반으로 응답을 생성함 장점: 빠르고, 간단하게 적용 가능 단점: 복잡한 문제나 구체적인 형식을 요구하는 작업에서는 모델의 응답이 기대에 못 미칠 수 있음 개선 방법 Few-Shot Prompting Memory 기능 사용 (피드백 기반 개선) 체인 결합 (여러 모델 연결) RAG (검색 후 생성) 퓨샷 프롬프팅 (Few-Shot Prompting) 몇 가지 예시를 프롬프트에 포함시켜 모델에게 원하는 출력 형식이나 해결 방식을 학습시키는 방식 특징 예시를 제공하여 모델이 어떤 방식으로 답변해야 하는지 안내 응답의 일관성과 정확도 향상 장점 원하는 결과를 얻을 확률이 높아짐 단점 적절한 예시 선택 실패 시 잘못된 결과 발생 가능 프롬프트가 길어질 수 있음 복잡한 질문 → 단계별 답변 (Few-Shot 활용) 예시 기반으로 단계별 사고 과정을 유도 모델이 단순한 답변 대신 추론 과정을 포함해 응답하도록 유도 가능 ExampleSelector 예시가 많을수록 토큰 사용량 증가 → 비용 상승 따라서 입력과 유사한 소수의 예시만 선택하는 전략 사용 방법: 예시와 입력 문장을 Text Embedding 후 코사인 유사도가 가장 높은 k개를 선택 콜백 (Callbacks) - 스트리밍(Streaming) 스트리밍: 입력에 대한 답변을 실시간 출력 완전한 응답이 생성되기 전에 중간 결과를 보여줌 방법 모델 객체 생성 시 StreamingStdOutCallbackHandler() 추가 오늘의 정리 Zero-Shot은 빠르고 단순하지만, 한계가 있음 Few-Shot은 예시 기반으로 정확도를 높이지만, 프롬프트 길이와 비용 문제가 있음 ExampleSelector로 효율적으로 예시를 선택 가능 Callback을 활용하면 실시간 스트리밍 방식으로 사용자 경험을 개선할 수 있음"
    },
    {
      "title": "Today I Learned ",
      "link": "https://velog.io/@goosego/Today-I-Learned",
      "pubDate": "Mon, 11 Aug 2025 07:02:44 GMT",
      "contentSnippet": "2025-08-11 텍스트 요약 (Text Summarization) 텍스트 요약의 개념: 문서나 기사의 핵심 내용을 유지하면서 길이를 줄이는 자연어 처리 태스크. 번역과 함께 대표적인 Seq2Seq 문제로 다뤄진다. 요약의 유형: 추출적 요약 (Extractive Summarization): 원본 문서에서 중요한 문장이나 구절을 추출하여 요약문을 생성한다. 생성적 요약 (Abstractive Summarization): 원본 내용을 이해하고 새로운 단어와 문장으로 요약문을 생성한다. 이번 실습에서는 생성적 요약을 다루었다. BillSum 데이터셋 활용 법안 텍스트와 그 요약으로 구성된 데이터셋을 사용했다. 특히 도메인 일반화 성능 평가를 위해 캘리포니아 주 법안 데이터(ca_test)를 활용하여 학습/평가 데이터셋을 분리했다. T5 모델을 이용한 생성적 요약 T5 (Text-to-Text Transfer Transformer) 모델을 사용했다. T5는 다양한 NLP 태스크를 '텍스트-투-텍스트' 문제로 통일하여 처리한다. 프롬프트 사용: T5 모델은 입력 텍스트 앞에 태스크를 나타내는 접두사(prefix)를 붙여 사용한다. 요약 태스크에서는 'summarize: '와 같은 형태의 prefix를 사용한다. 토큰화 (Tokenization): AutoTokenizer를 사용하여 텍스트를 모델이 이해할 수 있는 토큰 시퀀스로 변환했다. 입력(text)과 출력(summary, 레이블)의 최대 길이를 다르게 설정하고 패딩 및 잘림 처리를 했다. DataCollatorForSeq2Seq: Seq2Seq 모델 학습 시 배치(batch) 단위로 데이터를 처리할 때 입력 시퀀스와 레이블 시퀀스의 길이를 맞춰주기 위해 패딩을 수행하는 도구이다. 요약 모델 평가 (ROUGE Metric) ROUGE (Recall-Oriented Understudy for Gisting Evaluation): 생성된 요약문이 사람이 작성한 기준 요약문과 얼마나 유사한지를 측정하는 지표이다. 주요 ROUGE 지표: rouge1: 단어(unigram) 단위의 일치도를 측정. rouge2: 두 단어 쌍(bigram) 단위의 일치도를 측정하여 문맥 및 연속성을 평가. rougeL: 최장 공통 부분 수열(Longest Common Subsequence, LCS)을 기반으로 순서를 유지한 구조적 일치도를 평가. rougeLsum: 문장 단위의 LCS를 기반으로 구조적 유사도를 평가. compute_metrics 함수를 정의하여 예측값과 실제값을 문자열로 변환하고 ROUGE 점수를 계산하는 방법을 학습했다. 또한 생성된 요약의 평균 길이(gen_len)도 함께 측정했다. 모델 학습 및 배포 AutoModelForSeq2SeqLM을 사용하여 T5 모델을 로드했다. Seq2SeqTrainingArguments를 설정하여 학습 관련 하이퍼파라미터(학습률, 배치 크기, 에폭 수 등) 및 평가 전략을 정의했다. Seq2SeqTrainer를 사용하여 모델, 학습 인자, 데이터셋, 데이터 콜레이터, 평가 함수 등을 설정하고 모델을 학습시켰다. 학습된 모델을 Hugging Face Hub에 저장하고 업로드하여 공유 가능한 형태로 만들었다. 파이프라인을 이용한 요약 생성 transformers 라이브러리의 pipeline 기능을 사용하여 학습된 모델로 쉽게 요약 태스크를 수행할 수 있다. pipeline(\"summarization\", model=repo_id)를 통해 요약 파이프라인을 생성하고, 입력 텍스트를 넣어 요약 결과를 얻었다."
    },
    {
      "title": "KLUE - MRC 데이터셋을 이용한 질의응답(Question Answering) 모델 학습, \n",
      "link": "https://velog.io/@goosego/KLUE-MRC-%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%85%8B%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EC%A7%88%EC%9D%98%EC%9D%91%EB%8B%B5Question-Answering-%EB%AA%A8%EB%8D%B8-%ED%95%99%EC%8A%B5",
      "pubDate": "Thu, 07 Aug 2025 07:00:59 GMT",
      "contentSnippet": "2025-08-07 주요 학습 내용 데이터셋 로드 및 탐색: datasets 라이브러리를 사용하여 KLUE-MRC 데이터를 로드하고, 데이터의 구조(features, num_rows) 및 개별 예시의 구성(title, context, question, answers 등)을 확인했습니다. 데이터 전처리: Hugging Face transformers 라이브러리의 AutoTokenizer를 사용하여 'monologg/koelectra-small-discriminator' 체크포인트의 토크나이저를 로드했습니다. 질문(question)과 본문(context)을 함께 토큰화하고, max_length, truncation='only_second', padding='max_length', return_offsets_mapping=True 옵션을 설정하여 전처리 함수를 정의했습니다. offset_mapping 정보를 활용하여 원본 텍스트에서의 정답 위치(answer_start, text)를 토큰화된 시퀀스에서의 시작/끝 위치(start_positions, end_positions)로 변환하는 로직을 구현했습니다. 이 과정에서 정답이 context 범위를 벗어나는 경우 처리하는 예외 로직도 포함했습니다. 정의된 전처리 함수를 dataset.map() 메서드를 사용하여 전체 데이터셋에 적용하고 토큰화된 데이터셋(tokenized_sq)을 생성했습니다. 모델 로드 및 학습 준비: transformers 라이브러리의 AutoModelForQuestionAnswering를 사용하여 사전 학습된 'monologg/koelectra-small-discriminator' 모델을 질의응답 태스크에 맞게 로드했습니다. (이때, QA 레이어가 새로 초기화된다는 메시지를 확인했습니다.) Trainer 및 TrainingArguments 클래스를 사용하여 학습 파라미터(output_dir, eval_strategy, learning_rate, batch_size, num_train_epochs, weight_decay)를 설정하고 트레이너 객체를 생성했습니다. 모델 학습 및 허깅페이스 업로드: trainer.train() 메서드를 사용하여 모델 학습을 진행했습니다. huggingface_hub 라이브러리를 사용하여 허깅페이스에 로그인하고, 학습된 모델(model, tokenizer)을 지정된 repo_id(goosego/klue_mrc_koelectra_qa_model)로 저장하고 trainer.push_to_hub() 메서드를 통해 허깅페이스 허브에 업로드했습니다. 학습된 모델 추론 (테스트): 허깅페이스에 업로드된 체크포인트(goosego/klue_mrc_koelectra_qa_model)를 사용하여 transformers.pipeline으로 질의응답 파이프라인을 생성했습니다. 임의의 질문과 본문을 사용하여 모델 추론을 실행하고, 결과(answer, score)를 확인했습니다. 테스트 결과, 모델이 주어진 컨텍스트에서 질문에 대한 정확한 답변을 추출하지 못하고 낮은 신뢰도 값을 보였습니다. 이는 학습 데이터의 양이 적거나(1000개) 추가적인 튜닝이 필요함을 의미합니다."
    },
    {
      "title": "개체명 인식 학습 및 KLUE-MRC 데이터셋을 활용한 질의응답 (Question Answering)",
      "link": "https://velog.io/@goosego/TIL-KLUE-MRC-%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%85%8B%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-%EC%A7%88%EC%9D%98%EC%9D%91%EB%8B%B5-Question-Answering",
      "pubDate": "Wed, 06 Aug 2025 06:50:55 GMT",
      "contentSnippet": "2025-08-06 개체명 인식 (NER) 학습 개체명 인식(NER): 문장 내에서 사람, 위치, 조직, 날짜 등 특정 개체에 레이블을 할당하는 자연어 처리 태스크를 학습했습니다. KLUE-NER 데이터셋: 한국어 개체명 인식을 위해 Hugging Face 라이브러리를 통해 klue 데이터셋의 ner 부분을 로드하여 사용했습니다. 데이터셋은 문장, 토큰, BIO 기반의 정수 레이블로 구성되어 있습니다. BIO 태그: 개체명의 시작(B-), 내부(I-), 개체 아님(O)을 나타내는 BIO 태그 방식을 이해하고 데이터셋의 정수 라벨과 매핑하여 확인했습니다. Subword 토큰화: monologg/koelectra-base-v3-discriminator 토크나이저를 사용하여 Subword 토큰화의 개념과 필요성(OOV 문제 해결, 단어 조각 분석 용이)을 학습했습니다. 토큰화 및 라벨 정렬: tokenizer_and_labels 함수를 정의하여 입력 문장을 토큰화하고, word_ids()를 활용하여 Subword 토큰에 원래 단어의 NER 라벨을 올바르게 정렬하는 방법을 구현했습니다. 특히 -100 값을 사용하여 패딩 및 특수 토큰을 학습에서 제외 처리했습니다. 모델 학습: Hugging Face transformers 라이브러리의 AutoModelForTokenClassification을 사용하여 NER 모델을 로드하고, Trainer를 사용하여 모델 학습을 진행했습니다. 평가: seqeval 라이브러리를 사용하여 NER 모델의 성능을 평가하는 compute_metrics 함수를 정의했습니다. 모델 저장 및 업로드: 학습된 모델을 Hugging Face Hub에 로그인하여 저장하고 업로드했습니다. Pipeline 활용: Hugging Face pipeline을 사용하여 학습된 모델로 새로운 문장에 대한 개체명 인식을 수행하고 결과를 확인했습니다. 질의응답 (Question Answering) 태스크 질의응답은 주어진 질문에 대한 답변을 제공하는 자연어 처리 태스크입니다. 그 중 추출적(extractive) 질의응답은 주어진 문맥(context)에서 답변을 찾아 그대로 추출하는 방식입니다. KLUE - MRC 데이터셋 KLUE (Korean Language Understanding Evaluation) 데이터셋은 한국어 자연어 이해 태스크를 위한 벤치마크 데이터셋입니다. 그 중 MRC 데이터셋은 기계 독해 능력을 평가하기 위해 사용됩니다. 데이터셋의 주요 특징은 다음과 같습니다. title: 뉴스 기사의 제목 등 주제를 나타냅니다. context: 질문에 대한 답을 찾을 수 있는 본문 내용입니다. news_category: context의 뉴스 카테고리입니다. question: 모델이 답해야 하는 질문입니다. answers: 정답에 대한 정보입니다. answer_start: context에서 정답 텍스트가 시작되는 문자 위치입니다. text: 정답 텍스트입니다. is_impossible: 질문에 대한 답변이 context에 없는지 여부를 나타냅니다. question_type: 질문의 유형을 나타냅니다. source: 데이터 출처를 나타냅니다. guid: 고유 식별자입니다."
    },
    {
      "title": "임베딩 실습과 혐오 표현 텍스트 분류",
      "link": "https://velog.io/@goosego/%EC%9E%84%EB%B2%A0%EB%94%A9-%EC%8B%A4%EC%8A%B5%EA%B3%BC-%ED%98%90%EC%98%A4-%ED%91%9C%ED%98%84-%ED%85%8D%EC%8A%A4%ED%8A%B8-%EB%B6%84%EB%A5%98",
      "pubDate": "Tue, 29 Jul 2025 07:25:30 GMT",
      "contentSnippet": "2025-07-29 Part 1. 워드/문서 임베딩 및 유사도 계산 학습 목표 워드 임베딩(Word Embedding) 개념 이해 워드 임베딩 도구인 Word2Vec 이해 1. 워드 임베딩 (Word Embedding) 단어를 벡터(수치화된 값)로 변환하는 과정 텍스트 데이터 분석을 위해 의미적 유사성이 반영된 벡터 형태로 변경하는 것이 목표 2. Word2Vec 개별 단어를 벡터화하는 대표적인 워드 임베딩 방법 단어 간 유사도 계산 가능 (예: man – king + woman = queen) 학습 방식 CBOW (Continuous Bag-of-Words): 주변 단어로 중심 단어 예측 Skip-Gram: 중심 단어로 주변 단어 예측 (이번 실습에서 주로 사용) 3. 텍스트 데이터 전처리 kiwipiepy 형태소 분석기 설치 및 활용 리뷰 데이터에서 명사, 동사, 형용사 등 유의미한 품사만 추출 kiwi.tokenize()로 형태소 단위 토큰화 및 품사 태깅 추출된 품사에 따라 단어 필터링 후 저장 4. Word2Vec 모델 학습 (Gensim 활용) gensim 라이브러리 설치 및 불러오기 gensim.models.Word2Vec 모델 사용 window: 중심 단어 기준 앞뒤 학습할 단어 수 설정 sg: 학습 방식 선택 (Skip-Gram=1, CBOW=0) vector_size: 임베딩될 벡터 차원 설정 sentences: 학습할 토큰화된 문장 데이터 입력 5. Word Similarity 시각화 (Pyvis 활용) pyvis 라이브러리 설치 및 불러오기 Word2Vec.wv.most_similar()로 특정 단어와 유사한 단어 목록 및 유사도 획득 pyvis.network.Network로 유사도 네트워크 그래프 시각화 HTML 파일로 저장 후 브라우저에서 확인 6. Word2Vec 활용 예시 (음악 추천) 멜론 플레이리스트 데이터를 ‘문장’, 곡 ID를 ‘단어’로 간주 Word2Vec 모델 학습을 통해 곡(ID) 임베딩 생성 id2song, song2id 딕셔너리로 곡 ID↔제목 매핑 music_w2v.wv.most_similar()로 유사 곡 ID 목록 획득 후 제목 추천 7. Doc2Vec 문서를 고정 크기 벡터로 변환하는 도구 문서 간 유사도 계산에 주로 활용 단어 임베딩뿐 아니라 문서 전체 의미를 벡터로 표현 8. Doc2Vec 모델 학습 및 활용 (Gensim 활용) gensim.models.doc2vec.Doc2Vec, TaggedDocument, simple_preprocess 불러오기 각 리뷰(문서)에 고유 ID(인덱스) 태깅 위해 TaggedDocument 사용 simple_preprocess로 토큰화 및 특수문자 제거 Doc2Vec 학습 파라미터: vector_size, window, epochs 등 설정 d2v.dv.most_similar()로 특정 문서와 유사한 문서 목록 및 유사도 획득 Part 2. 혐오 표현 데이터 전처리 및 분류 학습 1. 데이터 로딩 및 탐색 tsv 형식의 혐오 표현 데이터셋(unsmile_train_v1.0.tsv, unsmile_valid_v1.0.tsv)을 Pandas DataFrame으로 불러옴 train.info(), train.head()로 데이터 기본 정보 및 샘플 확인 문장(X_train, X_test)과 레이블(y_train, y_test) 분리 2. 텍스트 데이터 전처리 한국어 자연어 처리를 위해 konlpy 라이브러리 사용 Okt 형태소 분석기로 문장을 형태소 단위로 분리 (norm=True, stem=True) 정규표현식(re)로 불필요 문자 제거 한 글자 토큰 제거 (len(token) < 2) 반복 자음·모음·숫자·영어 소문자·특정 기호 제거 emoji 라이브러리로 이모티콘 제거(정규표현식으로도 처리 가능) 레이블별 워드클라우드(wordcloud, matplotlib, koreanize-matplotlib)로 자주 등장 단어 시각화 3. 텍스트 데이터 벡터화 TF-IDF 방식으로 텍스트 수치화 sklearn.feature_extraction.text.TfidfVectorizer로 벡터 생성 훈련 데이터로 단어 사전 구축(fit), 훈련·테스트 데이터 변환(transform) 4. 모델 학습 및 예측 sklearn.linear_model.LogisticRegression으로 로지스틱 회귀 모델 생성 훈련 데이터로 모델 학습(fit) sklearn.model_selection.cross_val_score로 교차 검증 점수 확인 테스트 데이터 정확도 평가(score) 차원 불일치 오류 해결 과정 포함 요약 Word2Vec과 Doc2Vec을 통해 단어와 문서의 의미적 유사성을 벡터 공간에 표현했다. 음악 추천, 유사 단어 네트워크 시각화 등 워드 임베딩 활용 예시를 다뤘다. 혐오 표현 분류를 위해 데이터 로딩, 형태소 분석, 정규표현식 전처리, TF-IDF 벡터화, 로지스틱 회귀 모델 학습 과정을 실습했다. 앞으로의 확장 아이디어 사전 학습된 BERT, SBERT 임베딩으로 성능 비교하기 Transformer 기반 분류기나 LSTM, CNN 모델 적용해보기 멀티언어ㆍ다국적 데이터셋으로 일반화 실험 감성 분석, 주제 분류, 요약 생성 등 다른 NLP 태스크 확장"
    },
    {
      "title": "자연어 처리 과정 및 감성 분석",
      "link": "https://velog.io/@goosego/%EC%9E%90%EC%97%B0%EC%96%B4-%EC%B2%98%EB%A6%AC-%EA%B3%BC%EC%A0%95-%EB%B0%8F-%EA%B0%90%EC%84%B1-%EB%B6%84%EC%84%9D",
      "pubDate": "Mon, 28 Jul 2025 07:11:03 GMT",
      "contentSnippet": "2025-07-28 텍스트 데이터 분석 순서 텍스트 데이터 수집 분석에 필요한 텍스트 데이터를 수집 텍스트 전처리 오탈자 처리, 띄어쓰기 교정, 정규화 어간 추출, 표제어 추출 불용어 제거 (Stopword) 토큰화 (Tokenization) 단어 단위 (띄어쓰기 기준) 형태소 단위 (형태소 분석기 활용) 특징값 추출 (벡터화 및 수치화) 원핫인코딩 (One-Hot Encoding): 0/1로 단어 존재 여부 표현 (희소 행렬 발생 가능) BOW (Bag Of Words): 단어 등장 빈도 기반 CountVectorizer: 빈도 수 계산 TF-IDF: 문서 내 중요도를 가중치로 표현 데이터 분석 벡터화된 데이터를 기반으로 분석 모델 학습 및 평가 영화 리뷰 데이터 감성 분석 실습 1. 데이터 로드 ratings_train.txt, ratings_test.txt 불러오기 2. 데이터 확인 및 전처리 크기 확인, 결측치 제거 3. 단어 빈도 분석 및 워드클라우드 시각화 띄어쓰기 단위로 토큰화 → collections.Counter로 빈도 계산 WordCloud 라이브러리로 시각화 (한글 폰트 필요) 4. 형태소 분석 기반 워드클라우드 생성 사용 라이브러리: konlpy, kiwipiepy, Mecab 주요 품사 필터링: VV(동사), VA(형용사), NNG(일반명사) kiwi의 띄어쓰기 교정 기능 활용 5. 머신러닝 기반 감성 분석 수치화: CountVectorizer로 벡터화 (X_train, X_test) 모델링: sklearn.linear_model.LogisticRegression으로 학습 평가: cross_val_score, .score()를 사용한 정확도 확인 예측: 직접 작성한 문장으로 긍정/부정(1/0) 예측 수행 혐오표현 데이터 전처리 및 분류 1. 데이터 로드 unsmile_train_v1.0.tsv, unsmile_valid_v1.0.tsv 2. 이모지 제거 emoji 라이브러리로 제거 3. 형태소 분석 및 정제 사용 도구: konlpy의 Okt 옵션: norm=True, stem=True로 정규화 및 어간 추출 4. 불용어 제거 한 글자 토큰 제거 정규표현식: [ㅋㅎㄷㅇㅠㅜ?.!ㄱ-ㅎa-z0-9~><^-]+ 다음 단계 TF-IDF 기반 벡터화 혐오표현 분류 모델 학습 및 평가 예정"
    },
    {
      "title": "PyTorch를 이용한 심층 신경망(다중 분류)으로 손글씨 인식하기",
      "link": "https://velog.io/@goosego/PyTorch%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EC%8B%AC%EC%B8%B5-%EC%8B%A0%EA%B2%BD%EB%A7%9D%EB%8B%A4%EC%A4%91-%EB%B6%84%EB%A5%98%EC%9C%BC%EB%A1%9C-%EC%86%90%EA%B8%80%EC%94%A8-%EC%9D%B8%EC%8B%9D%ED%95%98%EA%B8%B0",
      "pubDate": "Tue, 22 Jul 2025 07:47:07 GMT",
      "contentSnippet": "2025-07-22 1. 라이브러리 및 데이터 준비 먼저 필요한 라이브러리를 가져옵니다. torch와 torchvision은 기본이고, 시각화를 위한 matplotlib, 데이터 처리를 위한 numpy, pandas, 그리고 모델 평가를 위한 sklearn 등을 사용합니다. # 라이브러리 import numpy as np import pandas as pd import matplotlib.pyplot as plt from copy import deepcopy from tqdm import tqdm import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torchvision import datasets, transforms from sklearn.model_selection import train_test_split from sklearn.metrics import confusion_matrix 평가 지표 소개 모델의 성능을 평가하기 위해 주로 사용되는 지표들입니다. 정확도 (Accuracy): 전체 예측값 중에서 정확히 맞춘 비율 재현율 (Recall): 실제 Positive 중에서 예측한 Positive의 비율 정밀도 (Precision): 예측한 Positive 중에서 실제 Positive의 비율 F1 스코어: 정밀도와 재현율의 조화평균 MNIST 데이터셋 로드 PyTorch의 torchvision을 사용하여 MNIST 데이터셋을 간편하게 불러옵니다. download=True로 설정하면 데이터가 없을 경우 자동으로 다운로드합니다. train = datasets.MNIST('./data', train = True, download = True, transform = transforms.Compose([transforms.ToTensor()])) test = datasets.MNIST('./data', train = False, download = True, transform = transforms.Compose([transforms.ToTensor()])) 데이터가 어떻게 생겼는지 확인하기 위해 첫 번째 이미지를 그려봅니다. def plot(x) : img = (np.array(x.detach().cpu(), dtype=\"float\")).reshape(28, 28) plt.figure(figsize=(3, 3)) plt.imshow(img, cmap=\"gray\") plt.show() plot(train.data[0]) 2. 데이터 전처리 모델에 입력하기 좋은 형태로 데이터를 가공하는 과정입니다. 정규화 및 데이터 형태 변환 정규화: 이미지 픽셀 값의 범위(0255)를 01 사이로 조정하여 학습을 안정화합니다. 형태 변환: 28x28 픽셀의 2차원 이미지를 784차원의 1차원 벡터로 펼쳐서 신경망의 입력으로 사용합니다. # 픽셀값을 0 ~ 1 사이의 값으로 정규화 X_data = train.data.float() / 255. y_data = train.targets X_test = test.data.float() / 255. y_test = test.targets print(\"초기 데이터 shape:\", X_data.shape, y_data.shape) # (28, 28) 2차원 데이터를 1차원 벡터로 변경 X_data = X_data.view(X_data.size(0), -1) X_test = X_test.view(X_test.size(0), -1) print(\"변환 후 데이터 shape:\", X_data.shape, y_data.shape) Train / Validation 데이터 분리 과적합을 방지하고 모델의 일반화 성능을 평가하기 위해, 학습 데이터를 다시 학습용(train)과 검증용(validation)으로 8:2 비율로 나눕니다. X_train, X_valid, y_train, y_valid = train_test_split(X_data, y_data, test_size = 0.2, random_state = 22) print(\"Train:\", X_train.shape, y_train.shape) print(\"Validation:\", X_valid.shape, y_valid.shape) print(\"Test:\", X_test.shape, y_test.shape) 모델 입출력 크기 설정 및 데이터 타입 변환 모델의 입력층과 출력층의 뉴런 수를 데이터에 맞게 설정하고, PyTorch의 CrossEntropyLoss가 요구하는 LongTensor 타입으로 정답 데이터의 타입을 변환합니다. # 입력 크기 설정 (이미지 벡터의 차원) input_size = X_train.size(-1) # 출력 크기 설정 (분류할 클래스의 개수, 0~9) output_size = int(max(y_train) + 1) print(f\"입력 크기: {input_size}, 출력 크기: {output_size}\") # 정답 데이터 타입을 LongTensor로 변환 y_train = y_train.long() y_valid = y_valid.long() y_test = y_test.long() 3. 모델 구축 심층 신경망(DNN) 모델 정의 nn.Sequential을 사용하여 여러 개의 선형 계층(Linear Layer)과 활성화 함수(ReLU)를 쌓아 심층 신경망을 정의합니다. 다중 분류 문제에서는 CrossEntropyLoss를 손실 함수로 사용할 경우, 출력층에 별도의 활성화 함수를 두지 않는 것이 일반적입니다. CrossEntropyLoss가 내부적으로 LogSoftmax와 NLLLoss를 포함하고 있기 때문입니다. # 다중분류에서는 출력층의 활성화함수를 작성하지 않는 방식을 권장 model = nn.Sequential( nn.Linear(input_size, 500), nn.ReLU(), nn.Linear(500, 400), nn.ReLU(), nn.Linear(400, 300), nn.ReLU(), nn.Linear(300, 200), nn.ReLU(), nn.Linear(200, 100), nn.ReLU(), nn.Linear(100, 50), nn.ReLU(), nn.Linear(50, output_size) ) print(model) GPU 설정 및 모델/데이터 이동 학습 속도 향상을 위해 GPU를 사용하도록 설정하고, 모델과 모든 데이터(train, valid, test)를 해당 장치로 이동시킵니다. device = torch.device('cpu') if torch.cuda.is_available(): device = torch.device('cuda:0') print('GPU 사용 가능, device:', device) else: print('GPU 사용 불가능, device:', device) # 모델과 데이터를 해당 device로 이동 model = model.to(device) X_train, y_train = X_train.to(device), y_train.to(device) X_valid, y_valid = X_valid.to(device), y_valid.to(device) X_test, y_test = X_test.to(device), y_test.to(device) 최적화 함수 및 손실 함수 설정 모델의 가중치를 업데이트할 Adam 옵티마이저와, 예측값과 실제값의 차이를 계산할 CrossEntropyLoss 손실 함수를 정의합니다. # 최적화함수 optimizer = optim.Adam(model.parameters()) # 손실함수 loss_func = nn.CrossEntropyLoss() 4. 모델 학습 학습 파라미터 설정 에포크 수, 배치 크기, 조기 종료(Early Stopping) 등 학습에 필요한 하이퍼파라미터를 설정합니다. n_epochs = 1000 batch_size = 256 print_interval = 10 lowest_loss = np.inf best_model = None early_stop = 50 lowest_epoch = np.inf 학습 루프 구현 본격적인 학습을 시작합니다. 학습 과정에서 검증 데이터(validation data)의 손실을 모니터링하여, 가장 성능이 좋은 모델(best_model)을 저장하고 과적합을 방지하기 위해 조기 종료를 적용합니다. train_history, valid_history = [], [] for epoch in tqdm(range(n_epochs)): # 1. 훈련 데이터 셔플 및 미니배치 분할 indices = torch.randperm(X_train.size(0)).to(device) X_ = torch.index_select(X_train, index = indices, dim = 0) y_ = torch.index_select(y_train, index = indices, dim = 0) X_ = X_.split(batch_size, dim = 0) y_ = y_.split(batch_size, dim = 0) train_loss = 0 # 2. 훈련 (Training) for X_i, y_i in zip(X_, y_): pre_y_i = model(X_i) loss = loss_func(pre_y_i, y_i) optimizer.zero_grad() loss.backward() optimizer.step() train_loss += float(loss) train_loss = train_loss / len(X_) # 3. 검증 (Validation) with torch.no_grad(): X_ = X_valid.split(batch_size, dim = 0) y_ = y_valid.split(batch_size, dim = 0) valid_loss = 0 for X_i, y_i in zip(X_, y_): pre_y_i = model(X_i) loss = loss_func(pre_y_i, y_i) valid_loss += float(loss) valid_loss = valid_loss / len(X_) # 4. 손실 기록 및 진행 상황 출력 train_history.append(train_loss) valid_history.append(valid_loss) if (epoch + 1) % print_interval == 0: print(f'epoch: {epoch + 1}, train_loss: {train_loss:.4e}, valid_loss: {valid_loss:.4e}, lowest_loss: {lowest_loss:.4e}') # 5. 베스트 모델 저장 if valid_loss <= lowest_loss: lowest_loss = valid_loss lowest_epoch = epoch best_model = deepcopy(model.state_dict()) # 6. 조기 종료 (Early Stopping) else: if early_stop > 0 and lowest_epoch + early_stop < epoch + 1: print(f'\\nEarly Stopping! {early_stop} epoch 동안 모델이 개선되지 않았습니다.') break # 7. 가장 성능이 좋았던 모델의 파라미터를 복원 model.load_state_dict(best_model) print(f'\\n{lowest_epoch + 1} epoch에서 검증 최저 손실 값({lowest_loss:.4e})을 달성했습니다.') 학습 손실 곡선 시각화 train_loss와 valid_loss를 시각화하여 학습이 잘 진행되었는지, 과적합은 발생하지 않았는지 확인합니다. plt.figure(figsize = (10, 6)) plt.grid(True) plt.plot(range(1, len(train_history) + 1), train_history, label = 'Train Loss') plt.plot(range(1, len(valid_history) + 1), valid_history, 'r',label = 'Validation Loss') plt.xlabel('Epoch') plt.ylabel('Loss') plt.legend() plt.yscale('log') # 손실 값의 변화를 자세히 보기 위해 y축을 로그 스케일로 설정 plt.show() 5. 모델 평가 테스트 데이터로 최종 평가 학습에 전혀 사용되지 않은 테스트 데이터로 최종 모델의 성능을 평가합니다. test_loss = 0 pre_y_test = [] with torch.no_grad(): X_ = X_test.split(batch_size, dim = 0) y_ = y_test.split(batch_size, dim = 0) for X_i, y_i in zip(X_, y_): pre_y_i = model(X_i) loss = loss_func(pre_y_i, y_i) test_loss += float(loss) pre_y_test.append(pre_y_i) test_loss = test_loss / len(X_) pre_y_test = torch.cat(pre_y_test, dim = 0) # 정확도 계산 corr_cnt = (y_test == torch.argmax(pre_y_test, dim = 1)).sum() total_cnt = float(y_test.size(0)) accuracy = corr_cnt / total_cnt print(f\"Test Loss: {test_loss:.4e}\") print(f\"Test Accuracy: {accuracy:.4f}\") Confusion Matrix (혼동 행렬) sklearn의 confusion_matrix를 사용하여 모델이 어떤 숫자를 잘 맞추고 어떤 숫자를 헷갈리는지 시각적으로 확인합니다. pd.DataFrame(confusion_matrix(y_test.cpu(), torch.argmax(pre_y_test.cpu(), dim = 1)), index = [f'True_{i}' for i in range(10)], columns = [f'Pred_{i}' for i in range(10)] ) Classification Report 정밀도, 재현율, f1-score를 클래스별로 한 번에 보여주는 classification_report로 더 상세한 성능을 확인합니다. from sklearn.metrics import classification_report y_true = y_test.cpu() y_pred = torch.argmax(pre_y_test.cpu(), dim = 1) report = classification_report(y_true, y_pred) print(report) 6. 직접 쓴 손글씨로 예측해보기 마지막으로, 그림판 등으로 직접 쓴 손글씨 이미지를 모델에 넣어 예측이 잘 되는지 테스트해봅니다. from PIL import Image # 1. 이미지 불러오기 및 전처리 # (주의) 이미지 경로는 실제 파일 위치에 맞게 수정해야 합니다. # (주의) 이미지는 배경이 흰색, 글씨가 검은색, 28x28 픽셀 크기일 때 가장 잘 인식됩니다. img_path = '/content/data/MNIST/손글씨3.png' transform = transforms.Compose([transforms.Grayscale(num_output_channels=1), transforms.ToTensor()]) try: img = Image.open(img_path) img_tensor = transform(img).to(device) # 모델 입력에 맞게 1차원 벡터로 변환 img_tensor = img_tensor.view(1, -1) # 2. 모델 예측 with torch.no_grad(): output = model(img_tensor) prediction = torch.argmax(output, dim=1) print(f\"'{img_path}' 이미지 예측 결과: {prediction.item()}\") except FileNotFoundError: print(f\"'{img_path}' 파일을 찾을 수 없습니다. 경로를 확인해주세요.\")"
    },
    {
      "title": "Pytorch로 심층신경망 설계 및 과대적합 방지 학습법",
      "link": "https://velog.io/@goosego/Pytorch%EB%A1%9C-%EC%8B%AC%EC%B8%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D-%EC%84%A4%EA%B3%84-%EB%B0%8F-%EA%B3%BC%EB%8C%80%EC%A0%81%ED%95%A9-%EB%B0%A9%EC%A7%80-%ED%95%99%EC%8A%B5%EB%B2%95",
      "pubDate": "Mon, 21 Jul 2025 07:15:21 GMT",
      "contentSnippet": "2025-07-22 학습 목표 Pytorch를 사용하여 심층신경망을 설계할 수 있다. 과대적합을 방지하기 위해 검증 데이터로 분리하여 학습할 수 있다. 학습 후 시각화를 통해 과적합 여부를 확인할 수 있다. 용어 정리 과대적합(Overfitting) 모델이 학습 데이터(train)에 지나치게 맞춰져서, 새로운 데이터(valid, test)에는 성능이 떨어지는 현상 과소적합(Underfitting) 모델이 학습 데이터조차 제대로 학습하지 못해 성능이 낮은 현상 일반화(Generalization) 학습 데이터뿐만 아니라 새로운 데이터에도 좋은 성능을 보이는 것 다양한 데이터셋에서 평균적으로 우수한 성능을 내는 것 데이터 분리 학습 데이터(train): 모델의 가중치를 업데이트하는 데 사용 검증 데이터(valid): 과대적합, 과소적합 여부를 확인할 때 사용 평가 데이터(test): 모델의 최종 성능을 평가할 때 사용 데이터 분리 순서 원본 데이터를 train과 test로 분리 train 데이터를 다시 train과 validation으로 분리 train 데이터로 모델 학습, validation 데이터로 검증 train과 validation 데이터를 합쳐서 최종 학습 test 데이터로 최종 평가 주요 개념 배치 학습 (Batch Learning) 전체 데이터를 한 번에 학습하지 않고, 일정 크기(batch_size)로 나누어 학습 batch_size: 한 번에 신경망에 넣는 데이터 묶음의 크기 iteration: 한 epoch 내에서 batch_size만큼 데이터를 학습하는 횟수 epoch: 전체 데이터를 한 번 모두 학습한 상태 예시: 100개의 데이터, batch_size=20 → 1 epoch에 5번의 iteration 1 epoch의 최종 loss는 모든 iteration의 평균값 학습 중 모델 저장 & 조기학습중단(Early Stopping) 모델 저장: 이전 epoch보다 성능이 개선(손실 감소)된 경우 모델을 저장 (과적합 방지, 최적의 모델 확보) 조기학습중단: 일정 횟수(epoch) 동안 검증 손실이 개선되지 않으면 학습을 중단 (불필요한 시간 낭비 방지) 학습 과정 요약 훈련 데이터를 랜덤으로 섞고, batch_size만큼 나눔 각 epoch마다 batch 단위로 학습 검증 데이터의 손실값(loss) 계산 및 저장 검증 손실이 최소값을 갱신하면 모델 저장 일정 횟수 이상 검증 손실이 개선되지 않으면 학습 중단 log 스케일을 사용하는 이유 값의 차이가 클 때, 작은 값들의 변화를 그래프에서 명확히 보기 위함 참고 Pytorch 공식 문서: https://pytorch.org/ Early Stopping 개념: https://en.wikipedia.org/wiki/Early_stopping"
    },
    {
      "title": "딥러닝 손실함수, 옵티마이저, 평가지표 정리",
      "link": "https://velog.io/@goosego/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%86%90%EC%8B%A4%ED%95%A8%EC%88%98-%EC%98%B5%ED%8B%B0%EB%A7%88%EC%9D%B4%EC%A0%80-%ED%8F%89%EA%B0%80%EC%A7%80%ED%91%9C-%EC%A0%95%EB%A6%AC",
      "pubDate": "Fri, 18 Jul 2025 12:02:14 GMT",
      "contentSnippet": "1. 손실함수(Loss Function) 손실함수는 모델의 예측값과 실제값의 차이(오차)를 수치로 계산하는 함수입니다. 손실함수의 값을 최소화하는 방향으로 모델이 학습됩니다. 문제 유형(회귀/이진분류/다중분류)에 따라 사용하는 손실함수가 다릅니다. 문제 유형 손실함수(loss) 설명 회귀 mean_squared_error (MSE) 예측값과 실제값의 제곱 오차 평균 이진분류 binary_crossentropy 두 클래스(0/1) 확률 예측 다중분류 categorical_crossentropy 원-핫 인코딩된 다중 클래스 다중분류(정수라벨) sparse_categorical_crossentropy 정수형 라벨(0,1,2...) 2. 옵티마이저(Optimizer) 옵티마이저는 손실함수를 최소화하기 위해 가중치와 바이어스를 어떻게 업데이트할지 결정하는 알고리즘입니다. 대표적으로 경사하강법(Gradient Descent) 계열이 많이 사용됩니다. 옵티마이저 특징 SGD 확률적 경사하강법, 기본적인 방식 Adam 학습률 자동 조정, 가장 많이 사용 RMSprop 순환 신경망(RNN)에서 자주 사용 Adagrad 희소 데이터에 적합 3. 평가지표(Metrics) 평가지표는 학습 과정이나 테스트에서 모델의 성능을 평가하는 기준입니다. 손실함수와는 별개로, 사람이 이해하기 쉬운 지표(정확도, MSE 등)를 추가로 확인할 수 있습니다. 문제 유형 평가지표(metrics) 설명 회귀 mse, mae, rmse 평균제곱오차, 평균절대오차 등 이진분류 accuracy 전체 중 맞춘 비율 다중분류 accuracy 전체 중 맞춘 비율 4. 예시 코드 # 이진분류 예시 model.compile( loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'] ) # 다중분류 예시 (원-핫 인코딩) model.compile( loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'] ) # 다중분류 예시 (정수 라벨) model.compile( loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'] ) # 회귀 예시 model.compile( loss='mean_squared_error', optimizer='adam', metrics=['mse'] ) 5. 정리 손실함수: 모델이 학습할 때 오차를 계산하는 기준 (문제 유형별로 다름) 옵티마이저: 오차를 줄이기 위해 가중치를 업데이트하는 방법 (Adam, SGD 등) 평가지표: 학습 및 평가 시 성능을 확인하는 지표 (accuracy, mse 등)"
    },
    {
      "title": "딥러닝 활성화 함수(Activation Function), 오차역전파(Back Propagation), 경사하강법(Gradient Descent Algorithm)",
      "link": "https://velog.io/@goosego/%EB%94%A5%EB%9F%AC%EB%8B%9D-%ED%99%9C%EC%84%B1%ED%99%94-%ED%95%A8%EC%88%98Activation-Function-%EC%A0%95%EB%A6%AC",
      "pubDate": "Fri, 18 Jul 2025 12:00:57 GMT",
      "contentSnippet": "1. 활성화 함수 (Activation Function) 1-1. 정의와 역할 활성화 함수는 신경망의 각 층에서 선형 연산 결과에 비선형성을 부여하는 함수입니다. 활성화 함수가 없다면, 아무리 층을 많이 쌓아도 결국 하나의 선형 함수와 동일한 결과만 나옴. 복잡한 패턴, 비선형 문제를 해결하기 위해 반드시 필요! 1-2. 주요 활성화 함수 종류 함수명 수식/특징 주요 사용처 Step 임계값 이상이면 1, 아니면 0 초기 퍼셉트론, 현재는 거의 사용 X Sigmoid ( f(x) = \\frac{1}{1+e^{-x}} ) 0~1 사이 값 이진분류 출력층, 은닉층(초기) Tanh ( f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} ) -1~1 사이 값 은닉층(초기) ReLU ( f(x) = \\max(0, x) ) 0 이하는 0, 0 초과는 그대로 현재 가장 많이 사용, 은닉층 Leaky ReLU ( f(x) = x ) (x>0), ( f(x) = 0.01x ) (x≤0) ReLU의 단점(죽은 뉴런) 보완 Softmax 각 클래스별 확률값(총합 1) 다중분류 출력층 1-3. 문제 유형별 출력층 활성화 함수 문제 유형 출력층 활성화 함수 units(출력 노드 수) 설명 회귀 Linear(항등함수) 1 연속형 값 예측, 활성화 함수 생략 가능 이진분류 Sigmoid 1 0~1 사이 확률값 출력 다중분류 Softmax 클래스 개수 각 클래스별 확률값 출력(총합 1) 2. 오차역전파 (Back Propagation) 2-1. 정의와 원리 오차역전파(Backpropagation)는 딥러닝 신경망이 학습할 때 사용하는 핵심 알고리즘입니다. 신경망의 출력값과 실제값의 오차(손실)를 계산한 뒤, 이 오차가 각 층(가중치, 바이어스)에 얼마나 영향을 미쳤는지 “역방향”으로 전달하여 가중치와 바이어스를 업데이트하는 과정입니다. 2-2. 학습 과정 순전파(Forward Propagation) 입력 데이터를 신경망에 통과시켜 예측값을 계산 손실 계산(Loss Calculation) 예측값과 실제값의 차이(손실함수)를 계산 역전파(Backpropagation) 손실이 각 가중치에 미치는 영향(기울기, Gradient)을 계산 체인룰(연쇄법칙)을 이용해, 출력층 → 은닉층 → 입력층 순서로 역방향으로 전달 가중치/바이어스 업데이트 계산된 기울기(Gradient)를 바탕으로, 옵티마이저(예: SGD, Adam)가 가중치와 바이어스를 업데이트 반복(Epochs) 위 과정을 여러 번 반복하며 모델의 성능을 점점 높임 2-3. 핵심 원리 체인룰(연쇄법칙, Chain Rule) 다층 신경망에서, 각 층의 가중치가 손실에 미치는 영향을 수학적으로 계산할 때 사용 기울기(Gradient) 계산 손실함수를 각 가중치로 미분하여, 손실이 가장 빠르게 줄어드는 방향(음의 기울기)으로 가중치를 조정 3. 경사하강법 (Gradient Descent Algorithm) 3-1. 정의와 역할 경사하강법(Gradient Descent)은 손실함수의 값을 최소화하는 방향(기울기, Gradient)으로 가중치와 바이어스를 반복적으로 업데이트하는 최적화 알고리즘입니다. 딥러닝에서 오차역전파로 계산된 기울기를 이용해, 경사하강법으로 파라미터를 조정합니다. 3-2. 동작 원리 손실함수의 기울기(Gradient) 계산 각 가중치에 대해 손실함수를 미분 기울기의 반대 방향으로 파라미터 업데이트 ( w = w - \\eta \\frac{\\partial L}{\\partial w} ) ( \\eta ): 학습률(Learning Rate) 반복(Epochs) 손실이 최소가 될 때까지 반복 3-3. 경사하강법의 종류 종류 특징 Batch Gradient Descent 전체 데이터로 한 번에 업데이트, 느리지만 안정적 Stochastic Gradient Descent (SGD) 데이터 한 개씩 업데이트, 빠르지만 불안정 Mini-batch Gradient Descent 여러 개씩 묶어서 업데이트, 가장 많이 사용 3-4. 한눈에 보는 경사하강법 손실함수의 값을 가장 빠르게 줄일 수 있는 방향(기울기)으로 파라미터를 조금씩 이동 학습률이 너무 크면 발산, 너무 작으면 학습이 느림 4. 전체 흐름 요약 순전파: 입력 → 각 층 통과 → 예측값 계산 (활성화 함수 적용) 손실 계산: 예측값과 실제값의 차이(손실함수) 역전파: 손실을 각 가중치에 대해 미분(기울기 계산, 체인룰) 경사하강법: 기울기 방향으로 가중치/바이어스 업데이트 반복: 여러 번 반복(Epochs)하며 성능 향상 5. 참고 실제 구현은 대부분 딥러닝 프레임워크(Keras, Pytorch 등)가 자동으로 처리해줍니다. 이 세 가지 원리가 딥러닝의 “학습”을 가능하게 하는 핵심입니다."
    },
    {
      "title": "딥러닝 모델링 과정",
      "link": "https://velog.io/@goosego/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EB%AA%A8%EB%8D%B8%EB%A7%81-%EA%B3%BC%EC%A0%95",
      "pubDate": "Fri, 18 Jul 2025 11:59:52 GMT",
      "contentSnippet": "1. 모델 구조 설계 딥러닝 모델을 만들 때 가장 먼저 해야 할 일은 모델의 구조(뼈대)를 설계하는 것입니다. 대표적인 모델 구조: Sequential Sequential() 모델은 층을 순차적으로 쌓는 가장 기본적인 구조입니다. from keras.models import Sequential from keras.layers import Dense model = Sequential() 각 층의 역할 층 종류 설명 입력층 입력 데이터의 형태(shape)를 지정 은닉층 연산을 통해 특징을 추출, units(노드 수)와 activation(활성화 함수) 지정 출력층 예측 결과의 형태를 지정, units와 activation 지정 예시 코드 model.add(Dense(units=64, activation='relu', input_shape=(8,))) model.add(Dense(units=32, activation='relu')) model.add(Dense(units=1, activation='sigmoid')) # 이진분류 예시 2. 모델 학습 방법 및 평가 방법 설정 (Compile) 모델을 학습시키기 전에 손실함수, 최적화 함수, 평가지표를 설정합니다. model.compile( loss='binary_crossentropy', # 손실함수 optimizer='adam', # 최적화 함수 metrics=['accuracy'] # 평가지표 ) 항목 설명 예시 loss 예측값과 실제값의 차이를 계산 회귀: MSE, 이진분류: binary_crossentropy optimizer 오차를 줄이는 방향으로 가중치 업데이트 Adam, SGD 등 metrics 학습 성능 평가 회귀: MSE, 분류: Accuracy 등 3. 모델 학습 및 시각화 (fit) 모델을 실제로 학습시키는 단계입니다. history = model.fit( X_train, y_train, epochs=50, batch_size=32, validation_split=0.2 ) 파라미터 설명 epochs 학습 반복 횟수 batch_size 데이터를 몇 개씩 나누어 학습할지 validation_split 검증 데이터 비율 학습 곡선 시각화 loss와 val_loss를 비교하여 학습 상태를 확인 val_loss가 감소하다가 다시 증가하면 과적합(Overfitting) 의심 import matplotlib.pyplot as plt plt.plot(history.history['loss'], label='Train Loss') plt.plot(history.history['val_loss'], label='Val Loss') plt.legend() plt.show() 4. 모델 예측 및 평가 예측: model.predict(X_test) 평가: model.evaluate(X_test, y_test) 5. 정리 모델 구조 설계: Sequential 등으로 층 쌓기 컴파일: 손실함수, 최적화 함수, 평가지표 설정 학습: fit()으로 데이터 학습, 학습 곡선 시각화 예측/평가: predict(), evaluate()로 성능 확인"
    },
    {
      "title": "다층 퍼셉트론(MLP, Multi-Layer Perceptron) ",
      "link": "https://velog.io/@goosego/%EB%8B%A4%EC%B8%B5-%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0MLP-Multi-Layer-Perceptron",
      "pubDate": "Fri, 18 Jul 2025 11:57:31 GMT",
      "contentSnippet": "1. 다층 퍼셉트론(MLP)이란? 다층 퍼셉트론(MLP)은 퍼셉트론(Perceptron)을 여러 층으로 쌓아 올린 인공신경망(ANN, Artificial Neural Network)입니다. 입력층(Input Layer), 하나 이상의 은닉층(Hidden Layer), 출력층(Output Layer)으로 구성됩니다. 비선형 활성화 함수를 사용하여, XOR 문제 등 단일 퍼셉트론으로는 해결할 수 없는 복잡한 문제도 해결할 수 있습니다. 2. 구조 입력층: 데이터가 처음 들어오는 부분 은닉층: 여러 개의 퍼셉트론이 모여 특징을 추출 출력층: 최종 예측 결과를 출력 예시 구조 (입력층) → (은닉층1) → (은닉층2) → (출력층) 3. MLP의 동작 원리 순전파(Forward Propagation) 입력값이 각 층을 거치며 가중치와 바이어스, 활성화 함수를 통해 출력값으로 전달됨 손실 계산(Loss Calculation) 예측값과 실제값의 차이를 손실함수로 계산 역전파(Backpropagation) 손실을 각 층에 역으로 전달하며, 가중치와 바이어스를 업데이트(경사하강법 등 사용) 반복 학습(Epochs) 위 과정을 여러 번 반복하며 모델의 성능을 높임 4. MLP의 특징 은닉층이 1개 이상: 단일 퍼셉트론과 달리, 복잡한 비선형 문제도 해결 가능 비선형 활성화 함수: ReLU, Sigmoid, Tanh 등 입력 데이터의 특징을 자동으로 추출 다양한 문제에 적용 가능: 분류, 회귀, 패턴 인식 등 5. MLP의 한계 은닉층이 많아질수록 학습이 어려워질 수 있음(기울기 소실/폭주 문제) 이미지, 음성 등 고차원 데이터에는 CNN, RNN 등 특화된 구조가 더 효과적 6. 간단한 MLP 구현 예시 (Keras) from keras.models import Sequential from keras.layers import Dense model = Sequential() model.add(Dense(64, activation='relu', input_shape=(입력차원,))) model.add(Dense(32, activation='relu')) model.add(Dense(출력차원, activation='softmax')) # 다중분류 예시 model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2) 7. 정리 다층 퍼셉트론(MLP)은 퍼셉트론을 여러 층으로 쌓아 복잡한 문제를 해결하는 신경망 구조 순전파, 손실 계산, 역전파, 반복 학습의 과정을 거쳐 모델이 점점 똑똑해짐 비선형 활성화 함수와 은닉층이 핵심"
    },
    {
      "title": "퍼셉트론(Perceptron)과 딥러닝 신경망 구조",
      "link": "https://velog.io/@goosego/%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0Perceptron%EA%B3%BC-%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%8B%A0%EA%B2%BD%EB%A7%9D-%EA%B5%AC%EC%A1%B0",
      "pubDate": "Fri, 18 Jul 2025 11:54:35 GMT",
      "contentSnippet": "1. 퍼셉트론(Perceptron)이란? 퍼셉트론은 딥러닝 신경망(Neural Network)을 구성하는 가장 작은 단위입니다. 하나의 퍼셉트론은 입력값에 가중치와 바이어스를 더한 뒤, 활성화 함수(Activation Function)를 거쳐 출력을 만듭니다. 퍼셉트론의 수식 [ y = f(w_1x_1 + w_2x_2 + ... + w_nx_n + b) ] ( x_1, x_2, ..., x_n ): 입력값 ( w_1, w_2, ..., w_n ): 가중치 ( b ): 바이어스 ( f ): 활성화 함수(예: sigmoid, relu 등) ( y ): 출력값 2. 퍼셉트론의 구조 3. 신경망(Neural Network)의 구조 퍼셉트론이 여러 개 모여 층(Layer)을 이루고, 여러 층이 쌓여 신경망(Neural Network)을 구성합니다. 입력층(Input Layer): 데이터가 처음 들어오는 부분 은닉층(Hidden Layer): 여러 개의 퍼셉트론이 모여 특징을 추출 출력층(Output Layer): 최종 예측 결과를 출력 4. 퍼셉트론의 한계와 다층 신경망 퍼셉트론 1개(단일층)는 XOR 문제 등 비선형 문제를 해결할 수 없음 여러 층(다층 퍼셉트론, MLP)을 쌓으면 복잡한 문제도 해결 가능 5. 활성화 함수(Activation Function) 퍼셉트론의 핵심은 활성화 함수입니다. 비선형성을 부여하여, 신경망이 복잡한 패턴을 학습할 수 있게 해줍니다. 함수명 특징 및 용도 Step 0/1로만 출력, 초기형태 Sigmoid 0~1 사이 값, 이진분류에 사용 ReLU 0 이하는 0, 0 초과는 그대로, 현재 가장 많이 사용 Softmax 다중분류의 출력층에 사용 6. 정리 퍼셉트론은 딥러닝 신경망의 기본 단위 여러 퍼셉트론이 모여 층을 이루고, 여러 층이 쌓여 신경망이 됨 활성화 함수로 비선형성을 부여해 복잡한 문제 해결 가능"
    },
    {
      "title": "딥러닝(Deep Learning) 개요",
      "link": "https://velog.io/@goosego/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%B4%EB%9E%80-Perceptron",
      "pubDate": "Fri, 18 Jul 2025 11:41:31 GMT",
      "contentSnippet": "1. 딥러닝이란? 딥러닝(Deep Learning)은 인간의 뇌 신경망(뉴런) 구조를 모방한 인공지능(AI) 기술입니다. 기존 머신러닝보다 더 깊고 복잡한 구조(다층 신경망, Deep Neural Network)를 사용하여, 데이터에서 직접 특징을 추출하고, 예측 및 분류, 생성 등 다양한 작업을 수행할 수 있습니다. 1-1. 딥러닝의 특징 다층 구조: 여러 개의 은닉층(Hidden Layer)으로 구성된 신경망 비선형성: 활성화 함수(Activation Function)를 통해 복잡한 패턴 학습 가능 특징 추출 자동화: 사람이 직접 특징을 설계하지 않아도, 데이터에서 중요한 특징을 스스로 학습 대용량 데이터에 강함: 데이터가 많을수록 성능이 좋아짐 GPU 등 고성능 컴퓨팅 환경에서 효과적 1-2. 딥러닝의 대표 활용 분야 이미지 인식/분류: 얼굴 인식, 자율주행 자동차, 의료 영상 분석 등 음성 인식: 스마트 스피커, 음성 비서 등 자연어 처리(NLP): 번역, 챗봇, 감정 분석 등 추천 시스템: 유튜브, 넷플릭스, 쇼핑몰 등 생성 모델: 이미지 생성, 텍스트 생성(GPT 등) 1-3. 딥러닝과 머신러닝의 차이 구분 머신러닝(Machine Learning) 딥러닝(Deep Learning) 구조 얕은 구조(1~2개 층) 깊은 구조(다수의 은닉층) 특징 추출 사람이 직접 설계(Feature Engineering) 자동으로 특징 추출 데이터 필요량 적은 데이터로도 가능 대용량 데이터 필요 대표 알고리즘 SVM, 결정트리, 랜덤포레스트 등 CNN, RNN, GAN, Transformer 등 2. 딥러닝의 기본 구조 딥러닝 모델은 입력층(Input Layer), 은닉층(Hidden Layer), 출력층(Output Layer)으로 구성됩니다. 입력층: 데이터가 처음 들어오는 부분 (예: 이미지의 픽셀값, 텍스트의 단어 벡터 등) 은닉층: 여러 층을 거치며 데이터의 특징을 추출 (딥러닝의 핵심) 출력층: 최종 예측 결과를 출력 (예: 분류 결과, 예측값 등) 3. 딥러닝의 핵심 구성 요소 퍼셉트론(Perceptron): 딥러닝 신경망의 가장 작은 단위(아래 글에서 자세히 설명) 가중치(Weight)와 바이어스(Bias): 학습을 통해 조정되는 값 활성화 함수(Activation Function): 비선형성을 부여하여 복잡한 패턴 학습 가능 손실 함수(Loss Function): 예측값과 실제값의 차이를 수치로 계산 최적화 알고리즘(Optimizer): 손실 함수를 최소화하는 방향으로 가중치/바이어스 업데이트 4. 딥러닝의 학습 과정 순전파(Forward Propagation): 입력 데이터를 신경망에 통과시켜 예측값 계산 손실 계산(Loss Calculation): 예측값과 실제값의 차이(오차) 계산 역전파(Backpropagation): 오차를 각 층에 역으로 전달하며 가중치/바이어스 업데이트 반복 학습(Epochs): 위 과정을 여러 번 반복하며 모델 성능 향상 5. 딥러닝의 한계와 과제 많은 데이터와 연산 자원 필요 과적합(Overfitting) 문제 설명력 부족(블랙박스) 하이퍼파라미터 튜닝의 어려움"
    }
  ]
}